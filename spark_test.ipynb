{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13be5d9e-008e-4895-8d2c-58d427f102e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b66e14-20d8-437d-a1aa-27f756b436ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b41bca5-e42b-4138-9ba7-9652013131b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e58dc4-9221-4ef9-91a2-482a7c1ef796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e57cc05-c6b8-41a2-b14c-56311c7b1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b79feb-0f05-4979-90d6-1efc92e20bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('testapp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b95e04e-d837-4aa0-b4af-18349dbc3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../Data/Dataset/FuelConsumption.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dadfe7e-9ef9-4bf5-8986-f53f02631bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+------------+----------+---------+------------+--------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "|      _c0|         _c1|         _c2|         _c3|       _c4|      _c5|         _c6|     _c7|                 _c8|                _c9|                _c10|                _c11|        _c12|\n",
      "+---------+------------+------------+------------+----------+---------+------------+--------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "|MODELYEAR|        MAKE|       MODEL|VEHICLECLASS|ENGINESIZE|CYLINDERS|TRANSMISSION|FUELTYPE|FUELCONSUMPTION_CITY|FUELCONSUMPTION_HWY|FUELCONSUMPTION_COMB|FUELCONSUMPTION_C...|CO2EMISSIONS|\n",
      "|     2014|       ACURA|         ILX|     COMPACT|         2|        4|         AS5|       Z|                 9.9|                6.7|                 8.5|                  33|         196|\n",
      "|     2014|       ACURA|         ILX|     COMPACT|       2.4|        4|          M6|       Z|                11.2|                7.7|                 9.6|                  29|         221|\n",
      "|     2014|       ACURA|  ILX HYBRID|     COMPACT|       1.5|        4|         AV7|       Z|                   6|                5.8|                 5.9|                  48|         136|\n",
      "|     2014|       ACURA|     MDX 4WD| SUV - SMALL|       3.5|        6|         AS6|       Z|                12.7|                9.1|                11.1|                  25|         255|\n",
      "|     2014|       ACURA|     RDX AWD| SUV - SMALL|       3.5|        6|         AS6|       Z|                12.1|                8.7|                10.6|                  27|         244|\n",
      "|     2014|       ACURA|         RLX|    MID-SIZE|       3.5|        6|         AS6|       Z|                11.9|                7.7|                  10|                  28|         230|\n",
      "|     2014|       ACURA|          TL|    MID-SIZE|       3.5|        6|         AS6|       Z|                11.8|                8.1|                10.1|                  28|         232|\n",
      "|     2014|       ACURA|      TL AWD|    MID-SIZE|       3.7|        6|         AS6|       Z|                12.8|                  9|                11.1|                  25|         255|\n",
      "|     2014|       ACURA|      TL AWD|    MID-SIZE|       3.7|        6|          M6|       Z|                13.4|                9.5|                11.6|                  24|         267|\n",
      "|     2014|       ACURA|         TSX|     COMPACT|       2.4|        4|         AS5|       Z|                10.6|                7.5|                 9.2|                  31|         212|\n",
      "|     2014|       ACURA|         TSX|     COMPACT|       2.4|        4|          M6|       Z|                11.2|                8.1|                 9.8|                  29|         225|\n",
      "|     2014|       ACURA|         TSX|     COMPACT|       3.5|        6|         AS5|       Z|                12.1|                8.3|                10.4|                  27|         239|\n",
      "|     2014|ASTON MARTIN|         DB9| MINICOMPACT|       5.9|       12|          A6|       Z|                  18|               12.6|                15.6|                  18|         359|\n",
      "|     2014|ASTON MARTIN|      RAPIDE|  SUBCOMPACT|       5.9|       12|          A6|       Z|                  18|               12.6|                15.6|                  18|         359|\n",
      "|     2014|ASTON MARTIN|  V8 VANTAGE|  TWO-SEATER|       4.7|        8|         AM7|       Z|                17.4|               11.3|                14.7|                  19|         338|\n",
      "|     2014|ASTON MARTIN|  V8 VANTAGE|  TWO-SEATER|       4.7|        8|          M6|       Z|                18.1|               12.2|                15.4|                  18|         354|\n",
      "|     2014|ASTON MARTIN|V8 VANTAGE S|  TWO-SEATER|       4.7|        8|         AM7|       Z|                17.4|               11.3|                14.7|                  19|         338|\n",
      "|     2014|ASTON MARTIN|V8 VANTAGE S|  TWO-SEATER|       4.7|        8|          M6|       Z|                18.1|               12.2|                15.4|                  18|         354|\n",
      "|     2014|ASTON MARTIN|    VANQUISH| MINICOMPACT|       5.9|       12|          A6|       Z|                  18|               12.6|                15.6|                  18|         359|\n",
      "+---------+------------+------------+------------+----------+---------+------------+--------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee52746b-3e14-4dd8-82d2-6c519ec7a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='MODELYEAR', _c1='MAKE', _c2='MODEL', _c3='VEHICLECLASS', _c4='ENGINESIZE', _c5='CYLINDERS', _c6='TRANSMISSION', _c7='FUELTYPE', _c8='FUELCONSUMPTION_CITY', _c9='FUELCONSUMPTION_HWY', _c10='FUELCONSUMPTION_COMB', _c11='FUELCONSUMPTION_COMB_MPG', _c12='CO2EMISSIONS')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74da4943-aa20-4f14-99f0-15a11f8404b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360e2985-d996-49a7-875c-08b62927d855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee122252-f3ca-47d6-bd82-826a40195994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e47ac1cf-e91e-4d53-86be-edd57c4845e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+----------+------------+------------------+------------------+------------+----+--------------------+-------------------+--------------------+--------------------+-----------------+\n",
      "|summary|      _c0|  _c1|       _c2|         _c3|               _c4|               _c5|         _c6| _c7|                 _c8|                _c9|                _c10|                _c11|             _c12|\n",
      "+-------+---------+-----+----------+------------+------------------+------------------+------------+----+--------------------+-------------------+--------------------+--------------------+-----------------+\n",
      "|  count|     1068| 1068|      1068|        1068|              1068|              1068|        1068|1068|                1068|               1068|                1068|                1068|             1068|\n",
      "|   mean|   2014.0| NULL|     300.0|        NULL|3.3462980318650346| 5.794751640112465|        NULL|NULL|  13.296532333645752|   9.47460168697282|  11.580880974695416|  26.441424554826618|256.2286785379569|\n",
      "| stddev|      0.0| NULL|      NULL|        NULL|1.4158950514240645|1.7974472750409625|        NULL|NULL|   4.101253317068338|  2.794510449885405|    3.48559484963484|   7.468701989863618|63.37230444279997|\n",
      "|    min|     2014|ACURA|1500 (MDS)|     COMPACT|                 1|                10|          A4|   D|                  10|                 10|                  10|                  11|              108|\n",
      "|    max|MODELYEAR|VOLVO|        xD|VEHICLECLASS|        ENGINESIZE|         CYLINDERS|TRANSMISSION|   Z|FUELCONSUMPTION_CITY|FUELCONSUMPTION_HWY|FUELCONSUMPTION_COMB|FUELCONSUMPTION_C...|     CO2EMISSIONS|\n",
      "+-------+---------+-----+----------+------------+------------------+------------------+------------+----+--------------------+-------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a statistical overview of the entire dataframe\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4806222e-aadf-492b-bdb3-c685156a805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      _c0|\n",
      "+---------+\n",
      "|MODELYEAR|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "|     2014|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a particular column\n",
    "df.select('_c0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "572fc109-3c9e-460f-a34d-3ea5a964fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='MODELYEAR', _c1='MAKE', _c2='MODEL', _c3='VEHICLECLASS', _c4='ENGINESIZE', _c5='CYLINDERS', _c6='TRANSMISSION', _c7='FUELTYPE', _c8='FUELCONSUMPTION_CITY', _c9='FUELCONSUMPTION_HWY', _c10='FUELCONSUMPTION_COMB', _c11='FUELCONSUMPTION_COMB_MPG', _c12='CO2EMISSIONS')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 5 rows of column 1\n",
    "df.head(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "677206e5-2b7c-453d-9f00-c7e2f53f2b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+\n",
      "|      _c0|         _c1|         _c2|\n",
      "+---------+------------+------------+\n",
      "|MODELYEAR|        MAKE|       MODEL|\n",
      "|     2014|       ACURA|         ILX|\n",
      "|     2014|       ACURA|         ILX|\n",
      "|     2014|       ACURA|  ILX HYBRID|\n",
      "|     2014|       ACURA|     MDX 4WD|\n",
      "|     2014|       ACURA|     RDX AWD|\n",
      "|     2014|       ACURA|         RLX|\n",
      "|     2014|       ACURA|          TL|\n",
      "|     2014|       ACURA|      TL AWD|\n",
      "|     2014|       ACURA|      TL AWD|\n",
      "|     2014|       ACURA|         TSX|\n",
      "|     2014|       ACURA|         TSX|\n",
      "|     2014|       ACURA|         TSX|\n",
      "|     2014|ASTON MARTIN|         DB9|\n",
      "|     2014|ASTON MARTIN|      RAPIDE|\n",
      "|     2014|ASTON MARTIN|  V8 VANTAGE|\n",
      "|     2014|ASTON MARTIN|  V8 VANTAGE|\n",
      "|     2014|ASTON MARTIN|V8 VANTAGE S|\n",
      "|     2014|ASTON MARTIN|V8 VANTAGE S|\n",
      "|     2014|ASTON MARTIN|    VANQUISH|\n",
      "+---------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['_c0','_c1','_c2']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e480acf-df8d-4e73-bd9b-53cbed7580dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+------------+----------+---------+------------+--------+--------------------+-------------------+--------------------+--------------------+------------+---------+\n",
      "|      _c0|         _c1|         _c2|         _c3|       _c4|      _c5|         _c6|     _c7|                 _c8|                _c9|                _c10|                _c11|        _c12|double_c0|\n",
      "+---------+------------+------------+------------+----------+---------+------------+--------+--------------------+-------------------+--------------------+--------------------+------------+---------+\n",
      "|MODELYEAR|        MAKE|       MODEL|VEHICLECLASS|ENGINESIZE|CYLINDERS|TRANSMISSION|FUELTYPE|FUELCONSUMPTION_CITY|FUELCONSUMPTION_HWY|FUELCONSUMPTION_COMB|FUELCONSUMPTION_C...|CO2EMISSIONS|     NULL|\n",
      "|     2014|       ACURA|         ILX|     COMPACT|         2|        4|         AS5|       Z|                 9.9|                6.7|                 8.5|                  33|         196|   4028.0|\n",
      "|     2014|       ACURA|         ILX|     COMPACT|       2.4|        4|          M6|       Z|                11.2|                7.7|                 9.6|                  29|         221|   4028.0|\n",
      "|     2014|       ACURA|  ILX HYBRID|     COMPACT|       1.5|        4|         AV7|       Z|                   6|                5.8|                 5.9|                  48|         136|   4028.0|\n",
      "|     2014|       ACURA|     MDX 4WD| SUV - SMALL|       3.5|        6|         AS6|       Z|                12.7|                9.1|                11.1|                  25|         255|   4028.0|\n",
      "|     2014|       ACURA|     RDX AWD| SUV - SMALL|       3.5|        6|         AS6|       Z|                12.1|                8.7|                10.6|                  27|         244|   4028.0|\n",
      "|     2014|       ACURA|         RLX|    MID-SIZE|       3.5|        6|         AS6|       Z|                11.9|                7.7|                  10|                  28|         230|   4028.0|\n",
      "|     2014|       ACURA|          TL|    MID-SIZE|       3.5|        6|         AS6|       Z|                11.8|                8.1|                10.1|                  28|         232|   4028.0|\n",
      "|     2014|       ACURA|      TL AWD|    MID-SIZE|       3.7|        6|         AS6|       Z|                12.8|                  9|                11.1|                  25|         255|   4028.0|\n",
      "|     2014|       ACURA|      TL AWD|    MID-SIZE|       3.7|        6|          M6|       Z|                13.4|                9.5|                11.6|                  24|         267|   4028.0|\n",
      "|     2014|       ACURA|         TSX|     COMPACT|       2.4|        4|         AS5|       Z|                10.6|                7.5|                 9.2|                  31|         212|   4028.0|\n",
      "|     2014|       ACURA|         TSX|     COMPACT|       2.4|        4|          M6|       Z|                11.2|                8.1|                 9.8|                  29|         225|   4028.0|\n",
      "|     2014|       ACURA|         TSX|     COMPACT|       3.5|        6|         AS5|       Z|                12.1|                8.3|                10.4|                  27|         239|   4028.0|\n",
      "|     2014|ASTON MARTIN|         DB9| MINICOMPACT|       5.9|       12|          A6|       Z|                  18|               12.6|                15.6|                  18|         359|   4028.0|\n",
      "|     2014|ASTON MARTIN|      RAPIDE|  SUBCOMPACT|       5.9|       12|          A6|       Z|                  18|               12.6|                15.6|                  18|         359|   4028.0|\n",
      "|     2014|ASTON MARTIN|  V8 VANTAGE|  TWO-SEATER|       4.7|        8|         AM7|       Z|                17.4|               11.3|                14.7|                  19|         338|   4028.0|\n",
      "|     2014|ASTON MARTIN|  V8 VANTAGE|  TWO-SEATER|       4.7|        8|          M6|       Z|                18.1|               12.2|                15.4|                  18|         354|   4028.0|\n",
      "|     2014|ASTON MARTIN|V8 VANTAGE S|  TWO-SEATER|       4.7|        8|         AM7|       Z|                17.4|               11.3|                14.7|                  19|         338|   4028.0|\n",
      "|     2014|ASTON MARTIN|V8 VANTAGE S|  TWO-SEATER|       4.7|        8|          M6|       Z|                18.1|               12.2|                15.4|                  18|         354|   4028.0|\n",
      "|     2014|ASTON MARTIN|    VANQUISH| MINICOMPACT|       5.9|       12|          A6|       Z|                  18|               12.6|                15.6|                  18|         359|   4028.0|\n",
      "+---------+------------+------------+------------+----------+---------+------------+--------+--------------------+-------------------+--------------------+--------------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column from existing column\n",
    "df  = df.withColumn('double_c0', df['_c0']*2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb1b045a-91c7-478a-83ef-7ac9a6c0ad6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.createOrReplaceGlobalTempView.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:176)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:174)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$2(BaseSessionStateBuilder.scala:155)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getRawGlobalTempView(SessionCatalog.scala:689)\r\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.$anonfun$run$4(views.scala:137)\r\n\tat org.apache.spark.sql.execution.command.ViewHelper$.createTemporaryViewRelation(views.scala:621)\r\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:141)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n\tat org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4363)\r\n\tat org.apache.spark.sql.Dataset.createOrReplaceGlobalTempView(Dataset.scala:3955)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Before we run sql commands we create a temporary view\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mcreateOrReplaceGlobalTempView(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuelConsumption\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sparkEnv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:480\u001b[0m, in \u001b[0;36mDataFrame.createOrReplaceGlobalTempView\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplaceGlobalTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates or replaces a global temporary view using the given name.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary view is tied to this Spark application.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcreateOrReplaceGlobalTempView(name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sparkEnv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sparkEnv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sparkEnv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.createOrReplaceGlobalTempView.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:176)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:174)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$2(BaseSessionStateBuilder.scala:155)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getRawGlobalTempView(SessionCatalog.scala:689)\r\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.$anonfun$run$4(views.scala:137)\r\n\tat org.apache.spark.sql.execution.command.ViewHelper$.createTemporaryViewRelation(views.scala:621)\r\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:141)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n\tat org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4363)\r\n\tat org.apache.spark.sql.Dataset.createOrReplaceGlobalTempView(Dataset.scala:3955)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Before we run sql commands we create a temporary view\n",
    "df.createOrReplaceGlobalTempView('FuelConsumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39c927-36ab-4110-bc09-5dedd75139b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
